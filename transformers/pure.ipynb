{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "17f99782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cbdf0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(v):\n",
    "    if isinstance(v, list):\n",
    "        return (len(v), ) + shape(v[0])\n",
    "    return ()\n",
    "\n",
    "def matmul(a, b):\n",
    "    if isinstance(a[0][0], list) and isinstance(b[0][0], list):\n",
    "        return [matmul(ra, rb) for ra, rb in zip(a, b)]\n",
    "    if isinstance(a[0][0], list) and not isinstance(b[0][0], list):\n",
    "        return [matmul(ra, b) for ra in a]\n",
    "    if not isinstance(a[0][0], list) and isinstance(b[0][0], list):\n",
    "        return [matmul(a, rb) for rb in b]\n",
    "    \n",
    "    assert len(a[0]) == len(b)\n",
    "\n",
    "    d1 = len(a)\n",
    "    d2 = len(b[0])\n",
    "    d3 = len(b)\n",
    "\n",
    "    c = [[0.0] * d2 for _ in range(d1)]\n",
    "\n",
    "    for i in range(d1):\n",
    "        for j in range(d2):\n",
    "            for k in range(d3):\n",
    "                c[i][j] += a[i][k] * b[k][j]\n",
    "    \n",
    "    return c\n",
    "def scalarmul(v, c):\n",
    "    if isinstance(v, list):\n",
    "        return [scalarmul(row, c) for row in v]\n",
    "    return v * c\n",
    "    \n",
    "def scalaradd(v, c):\n",
    "    if isinstance(v, list):\n",
    "        return [scalaradd(row, c) for row in v]\n",
    "    return v + c\n",
    "\n",
    "def matadd(a, b):\n",
    "    if isinstance(a[0][0], list) and isinstance(b[0][0], list):\n",
    "        return [matadd(ra, rb) for ra, rb in zip(a, b)]\n",
    "    if isinstance(a[0][0], list) and not isinstance(b[0][0], list):\n",
    "        return [matadd(ra, b) for ra in a]\n",
    "    if not isinstance(a[0][0], list) and isinstance(b[0][0], list):\n",
    "        return [matadd(a, rb) for rb in b]\n",
    "    # both 2D\n",
    "    assert len(a[0]) == len(b[0])\n",
    "\n",
    "    is_vector = (len(b) == 1)\n",
    "    if not is_vector:\n",
    "        assert len(a) == len(b)\n",
    "    \n",
    "    d1 = len(a)\n",
    "    d2 = len(a[0])\n",
    "\n",
    "    c = [[0.0] * d2 for _ in range(d1)]\n",
    "\n",
    "    for i in range(d1):\n",
    "        for j in range(d2):\n",
    "            c[i][j] = a[i][j] + b[i][j] if not is_vector else b[0][j]\n",
    "    \n",
    "    return c\n",
    "def transpose(v):\n",
    "    if isinstance(v[0][0], list):\n",
    "        return [transpose(row) for row in v]\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len(v[0])):\n",
    "        res.append([])\n",
    "        for j in range(len(v)):\n",
    "            res[i].append(v[j][i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a808257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.1):\n",
    "        self.p = p\n",
    "        self.scale = 1.0 / (1.0 - self.p) if p < 1 else 0\n",
    "        \n",
    "    def execute(self, v):\n",
    "        if isinstance(v, list):\n",
    "            return [self.execute(row) for row in v]\n",
    "        \n",
    "        ret = 0.0 if random.random() < self.p else v * self.scale\n",
    "\n",
    "        return ret\n",
    "\n",
    "def softmax(v): # axis = -1\n",
    "    if isinstance(v[0], list):\n",
    "        return [softmax(row) for row in v]\n",
    "\n",
    "    big = max(v)\n",
    "    exps = [math.exp(x - big) for x in v]\n",
    "    tot = sum(exps)\n",
    "\n",
    "    return [x / tot for x in exps]\n",
    "\n",
    "def relu(v):\n",
    "    if isinstance(v, list):\n",
    "        return [relu(row) for row in v]\n",
    "    return max(0, v)\n",
    "\n",
    "def layer_normalization(v, gamma, beta, epsilon=1e-5):\n",
    "    if isinstance(v[0], list):\n",
    "        return [layer_normalization(row, gamma, beta, epsilon) for row in v]\n",
    "    \n",
    "    mean = sum(v) / len(v)\n",
    "    std = math.sqrt(sum([(x - mean) ** 2 for x in v]) / len(v))\n",
    "\n",
    "    x = scalaradd(v, -mean)\n",
    "    x = [val * g / (std + epsilon) + b for val, g, b in zip(x, gamma[0], beta[0])]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7788d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_encoding(seq_len, d_model):\n",
    "    pe = []\n",
    "    for pos in range(seq_len):\n",
    "        pe.append([])\n",
    "        for i in range(d_model // 2):\n",
    "            pe[pos].append(math.sin(pos / (10000 ** (2 * i / d_model))))\n",
    "            pe[pos].append(math.cos(pos / (10000 ** (2 * i / d_model))))\n",
    "    return pe\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    return matmul(X, W_q), matmul(X, W_k), matmul(X, W_v)\n",
    "\n",
    "def get_causal_mask(generated_len):\n",
    "    v = [[1] * generated_len for _ in range(generated_len)]\n",
    "    for i in range(0, generated_len):\n",
    "        for j in range(i + 1, generated_len):\n",
    "            v[i][j] = 0\n",
    "    return v\n",
    "    \n",
    "def self_attention(Q, K, V, mask = None, dropout=None):\n",
    "    d_k = shape(Q)[-1]\n",
    "    score = scalarmul(matmul(Q, transpose(K)), (1.0 / math.sqrt(d_k)))\n",
    "    # mask: (seq_len, seq_len)\n",
    "\n",
    "    if mask is not None:\n",
    "        for i in range(len(score)):\n",
    "            for j in range(len(score[0])):\n",
    "                if mask[i][j] == 0:\n",
    "                    score[i][j] = -1e9\n",
    "\n",
    "    pscore = softmax(score)\n",
    "\n",
    "    if dropout is not None:\n",
    "        pscore = dropout.execute(pscore)\n",
    "\n",
    "    return matmul(pscore, V)\n",
    "\n",
    "def split_for_head(X, n_heads):\n",
    "    # seq_len, d_model -> n_heads, seq_len, d_head\n",
    "\n",
    "    d_model = shape(X)[-1]\n",
    "    d_head = d_model // n_heads\n",
    "    \n",
    "    heads = []\n",
    "    for i in range(n_heads):\n",
    "        head = []\n",
    "        for row in X:\n",
    "            subseq = row[i * d_head : (i + 1) * d_head]\n",
    "            head.append(subseq)\n",
    "        heads.append(head)\n",
    "    return heads\n",
    "        \n",
    "\n",
    "def horizontal_concat(X):\n",
    "    # n_heads, seq_len, d_head -> seq_len, d_model\n",
    "    res = []\n",
    "    for i in range(len(X[0])):\n",
    "        full_row = []\n",
    "        for head in X:\n",
    "            full_row.extend(head[i])\n",
    "        res.append(full_row)\n",
    "\n",
    "    return res\n",
    "\n",
    "def multi_head_attention(Q, K, V, W_o, n_heads, mask=None, dropout=None):\n",
    "    # seq_len, d_model\n",
    "    \n",
    "    Q_split = split_for_head(Q, n_heads)\n",
    "    K_split = split_for_head(K, n_heads)\n",
    "    V_split = split_for_head(V, n_heads)\n",
    "\n",
    "    # n_head, seq_len, d_head\n",
    "    heads = [self_attention(Q_split[i], K_split[i], V_split[i], mask=mask, dropout=dropout) for i in range(n_heads)]\n",
    "    \n",
    "    concat_heads = horizontal_concat(heads)\n",
    "\n",
    "    # seq_len, d_model\n",
    "    return matmul(concat_heads, W_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7e0f2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_blocks):\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.embedding = self.init_weights(vocab_size, d_model)\n",
    "        self.encoders = [self.create_block(is_decoder=False) for _ in range(n_blocks)]\n",
    "        self.decoders = [self.create_block(is_decoder=True) for _ in range(n_blocks)]\n",
    "\n",
    "        self.dropout_p = 0.1\n",
    "        self.train()\n",
    "    \n",
    "    def init_weights(self, rows, cols):\n",
    "        limit = math.sqrt(6 / (rows + cols))\n",
    "        v = []\n",
    "        for i in range(rows):\n",
    "            v.append([random.uniform(-limit, +limit) for _ in range(cols)])\n",
    "        return v\n",
    "\n",
    "    def create_block(self, is_decoder):\n",
    "        weights = {\n",
    "            'W_q': self.init_weights(self.d_model, self.d_model),\n",
    "            'W_k': self.init_weights(self.d_model, self.d_model),\n",
    "            'W_v': self.init_weights(self.d_model, self.d_model),\n",
    "            'W_o': self.init_weights(self.d_model, self.d_model),\n",
    "            'W_ff1': self.init_weights(self.d_model, self.d_ff),\n",
    "            'bias_ff1': [[0] * self.d_ff],\n",
    "            'W_ff2': self.init_weights(self.d_ff, self.d_model),\n",
    "            'bias_ff2': [[0] * self.d_model],\n",
    "            'gamma': [[1] * self.d_model],\n",
    "            'beta': [[0] * self.d_model],\n",
    "        }\n",
    "        if is_decoder:\n",
    "            weights.update({\n",
    "                'W_q_cross': self.init_weights(self.d_model, self.d_model),\n",
    "                'W_k_cross': self.init_weights(self.d_model, self.d_model),\n",
    "                'W_v_cross': self.init_weights(self.d_model, self.d_model),\n",
    "                'W_o_cross': self.init_weights(self.d_model, self.d_model),\n",
    "            })\n",
    "        return weights\n",
    "    \n",
    "    def run_encoders(self, x):\n",
    "        seq_len = shape(x)[-1]\n",
    "        # [seq_len]\n",
    "        x = scalarmul([self.embedding[idx] for idx in x], math.sqrt(self.d_model))\n",
    "        # [seq_len x d_model]\n",
    "        x = matadd(x, pos_encoding(seq_len, self.d_model))\n",
    "\n",
    "        if self.dropout: x = self.dropout.execute(x)\n",
    "\n",
    "        for weights in self.encoders:\n",
    "\n",
    "            Q, K, V = compute_qkv(x, weights['W_q'], weights['W_k'], weights['W_v'])\n",
    "            att = multi_head_attention(Q, K, V, weights['W_o'], self.n_heads, mask=None, dropout=self.dropout)\n",
    "            if self.dropout: att = self.dropout.execute(att)\n",
    "            x = layer_normalization(matadd(x, att), weights['gamma'], weights['beta'])\n",
    "            \n",
    "            ff = matadd(matmul(relu(matadd(matmul(x, weights['W_ff1']), weights['bias_ff1'])), weights['W_ff2']), weights['bias_ff2'])\n",
    "            if self.dropout: ff = self.dropout.execute(ff)\n",
    "            x = layer_normalization(matadd(x, ff), weights['gamma'], weights['beta'])\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def run_decoders(self, enc_out, x):\n",
    "        seq_len = shape(x)[-1]\n",
    "\n",
    "        x = scalarmul([self.embedding[idx] for idx in x], math.sqrt(self.d_model))\n",
    "        x = matadd(x, pos_encoding(seq_len, self.d_model))\n",
    "        \n",
    "        if self.dropout: x = self.dropout.execute(x)\n",
    "\n",
    "        mask = get_causal_mask(seq_len)\n",
    "\n",
    "        for weights in self.decoders:\n",
    "\n",
    "            Q, K, V = compute_qkv(x, weights['W_q'], weights['W_k'], weights['W_v'])\n",
    "            att = multi_head_attention(Q, K, V, weights['W_o'], self.n_heads, mask=mask, dropout=self.dropout)\n",
    "            if self.dropout: att = self.dropout.execute(att)\n",
    "            x = layer_normalization(matadd(x, att), weights['gamma'], weights['beta'])\n",
    "\n",
    "            Q = matmul(x, weights['W_q_cross'])\n",
    "            K = matmul(enc_out, weights['W_k_cross'])\n",
    "            V = matmul(enc_out, weights['W_v_cross'])\n",
    "            cross_att = multi_head_attention(Q, K, V, weights['W_o_cross'], self.n_heads, mask=None, dropout=self.dropout)\n",
    "            if self.dropout: cross_att = self.dropout.execute(cross_att)\n",
    "            x = layer_normalization(matadd(x, cross_att), weights['gamma'], weights['beta'])\n",
    "            \n",
    "            ff = matadd(matmul(relu(matadd(matmul(x, weights['W_ff1']), weights['bias_ff1'])), weights['W_ff2']), weights['bias_ff2'])\n",
    "            if self.dropout: ff = self.dropout.execute(ff)\n",
    "            x = layer_normalization(matadd(x, ff), weights['gamma'], weights['beta'])\n",
    "\n",
    "        return matmul(x, transpose(self.embedding)) # seq_len x vocab_size\n",
    "    \n",
    "    def generate(self, src, start_token, max_len):\n",
    "        enc_out = self.run_encoders(src)\n",
    "\n",
    "        output = [start_token]\n",
    "        for _ in range(max_len):\n",
    "            logits = self.run_decoders(enc_out, output)\n",
    "            probs = softmax(logits[-1])\n",
    "            prob, token = max([(x, idx) for idx, x in enumerate(probs)])\n",
    "            output.append(token)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def eval(self):\n",
    "        self.dropout = None\n",
    "    def train(self):\n",
    "        self.dropout = Dropout(p=self.dropout_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "98d863ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 20, 19, 14, 9, 17, 3, 13, 19, 1] [0, 20, 20, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "[4, 8, 6, 11, 5, 17, 16, 15, 6, 17] [0, 20, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[18, 12, 17, 18, 18, 13, 10, 13, 4, 2] [0, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "[18, 19, 9, 11, 6, 1, 5, 2, 2, 3] [0, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "[13, 18, 12, 15, 18, 2, 1, 6, 13, 8] [0, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "[7, 4, 17, 4, 10, 18, 10, 10, 19, 14] [0, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "[2, 18, 7, 4, 1, 5, 9, 16, 5, 20] [0, 20, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[16, 20, 6, 20, 15, 5, 11, 18, 4, 10] [0, 20, 20, 20, 20, 20, 20, 20, 3, 3, 3]\n",
      "[2, 5, 2, 19, 9, 19, 12, 19, 14, 4] [0, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "[9, 15, 20, 17, 7, 4, 8, 5, 14, 3] [0, 20, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "\n",
    "model = Transformer(vocab_size=21, d_model=128, n_heads=4, d_ff=256, n_blocks=2)\n",
    "model.eval()\n",
    "\n",
    "for _ in range(10):\n",
    "    src = [random.randint(1, 20) for _ in range(10)]\n",
    "    print(src, model.generate(src, 0, len(src)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
