{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a808257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, p=0.1):\n",
    "        self.p = p\n",
    "        \n",
    "    def execute(self, v):\n",
    "        scale = 1.0 / (1.0 - self.p)\n",
    "        mask = np.random.random(v.shape)\n",
    "        mask = np.where(mask < self.p, 0.0, scale)\n",
    "\n",
    "        return v * mask\n",
    "\n",
    "def softmax(v):\n",
    "    v = np.exp(v - np.max(v, axis=-1, keepdims=True))\n",
    "    tot = np.sum(v, axis=-1, keepdims=True)\n",
    "    return v / tot\n",
    "\n",
    "def relu(v):\n",
    "    return np.maximum(0, v)\n",
    "\n",
    "def layer_normalization(X, gamma, beta, epsilon=1e-5):\n",
    "    mean = np.mean(X, axis = -1, keepdims=True)\n",
    "    std = np.std(X, axis = -1, keepdims=True)\n",
    "    return (X - mean) * gamma / (std + epsilon) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7788d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pos_encoding(seq_len, d_model):\n",
    "    if seq_len == 0 or d_model <= 0:\n",
    "        return -1\n",
    "\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "\n",
    "    i = np.arange(0, d_model // 2 + d_model % 2)\n",
    "    divisor = np.power(10000, i * 2 / d_model)\n",
    "\n",
    "    position = np.arange(seq_len)[:, None]\n",
    "    pe[:, 0::2] = np.sin(position / divisor)\n",
    "    pe[:, 1::2] = np.cos(position / (divisor if d_model % 2 == 0 else divisor[:-1]))\n",
    "\n",
    "    return pe\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    return X @ W_q, X @ W_k, X @ W_v\n",
    "\n",
    "def get_causal_mask(generated_len):\n",
    "    return np.tril(np.ones((generated_len, generated_len)))\n",
    "\n",
    "def self_attention(Q, K, V, mask = None, dropout=None):\n",
    "    d_k = Q.shape[-1]\n",
    "    score = Q @ K.swapaxes(-1, -2) / np.sqrt(d_k)\n",
    "    # mask: (seq_len, seq_len)\n",
    "\n",
    "    if mask is not None:\n",
    "        score = np.where(mask == 1, score, -1e9)\n",
    "\n",
    "    pscore = softmax(score)\n",
    "\n",
    "    return pscore @ V\n",
    "\n",
    "def multi_head_attention(Q, K, V, W_o, n_heads, mask=None, dropout=None):\n",
    "    # seq_len, d_model\n",
    "    seq_len, d_model = Q.shape\n",
    "    d_head = d_model // n_heads\n",
    "\n",
    "    shape1 = Q.shape[:-1] + (n_heads, d_head)\n",
    "    shape2 = K.shape[:-1] + (n_heads, d_head)\n",
    "    # ser_len, n_head, d_head\n",
    "    Q = Q.reshape(shape1).swapaxes(-2, -3)\n",
    "    K = K.reshape(shape2).swapaxes(-2, -3)\n",
    "    V = V.reshape(shape2).swapaxes(-2, -3)\n",
    "    \n",
    "    # n_head, seq_len, d_head\n",
    "    ret = self_attention(Q, K, V, mask=mask, dropout=dropout)\n",
    "    \n",
    "    concat_heads = ret.swapaxes(0, 1).reshape(seq_len, d_model)\n",
    "    # seq_len, d_model\n",
    "    return concat_heads @ W_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_blocks):\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.embedding = self.init_weights(vocab_size, d_model)\n",
    "        self.encoders = [self.create_block(is_decoder=False) for _ in range(n_blocks)]\n",
    "        self.decoders = [self.create_block(is_decoder=True) for _ in range(n_blocks)]\n",
    "\n",
    "        self.dropout_p = 0.1\n",
    "        self.train()\n",
    "    \n",
    "    def init_weights(self, rows, cols):\n",
    "        limit = np.sqrt(6 / (rows + cols))\n",
    "        return np.random.uniform(-limit, limit, (rows, cols))\n",
    "\n",
    "    def create_block(self, is_decoder):\n",
    "        weights = {\n",
    "            'W_q': self.init_weights(self.d_model, self.d_model),\n",
    "            'W_k': self.init_weights(self.d_model, self.d_model),\n",
    "            'W_v': self.init_weights(self.d_model, self.d_model),\n",
    "            'W_o': self.init_weights(self.d_model, self.d_model),\n",
    "            'W_ff1': self.init_weights(self.d_model, self.d_ff),\n",
    "            'bias_ff1': np.zeros((1, self.d_ff)),\n",
    "            'W_ff2': self.init_weights(self.d_ff, self.d_model),\n",
    "            'bias_ff2': np.zeros((1, self.d_model)),\n",
    "            'gamma': np.ones((1, self.d_model)),\n",
    "            'beta': np.zeros((1, self.d_model)),\n",
    "        }\n",
    "        if is_decoder:\n",
    "            weights.update({\n",
    "                'W_q_cross': self.init_weights(self.d_model, self.d_model),\n",
    "                'W_k_cross': self.init_weights(self.d_model, self.d_model),\n",
    "                'W_v_cross': self.init_weights(self.d_model, self.d_model),\n",
    "                'W_o_cross': self.init_weights(self.d_model, self.d_model),\n",
    "            })\n",
    "        return weights\n",
    "    \n",
    "    def run_encoders(self, x):\n",
    "        x = np.array(x)\n",
    "        seq_len = x.shape[-1]\n",
    "        # [seq_len]\n",
    "        x = self.embedding[x] * np.sqrt(self.d_model)\n",
    "        # [seq_len x d_model]\n",
    "        x += pos_encoding(seq_len, self.d_model)\n",
    "\n",
    "        if self.dropout: x = self.dropout.execute(x)\n",
    "\n",
    "        for weights in self.encoders:\n",
    "\n",
    "            Q, K, V = compute_qkv(x, weights['W_q'], weights['W_k'], weights['W_v'])\n",
    "            att = multi_head_attention(Q, K, V, weights['W_o'], self.n_heads, mask=None, dropout=self.dropout)\n",
    "            if self.dropout: att = self.dropout.execute(att)\n",
    "            x = layer_normalization(x + att, weights['gamma'], weights['beta'])\n",
    "            \n",
    "            ff = relu(x @ weights['W_ff1'] + weights['bias_ff1']) @ weights['W_ff2'] + weights['bias_ff2']\n",
    "            if self.dropout: ff = self.dropout.execute(ff)\n",
    "            x = layer_normalization(x + ff, weights['gamma'], weights['beta'])\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def run_decoders(self, enc_out, x):\n",
    "        x = np.array(x)\n",
    "        seq_len = x.shape[-1]\n",
    "\n",
    "        x = self.embedding[x] * np.sqrt(self.d_model)\n",
    "        x += pos_encoding(seq_len, self.d_model)\n",
    "        \n",
    "        if self.dropout: x = self.dropout.execute(x)\n",
    "\n",
    "        mask = get_causal_mask(seq_len)\n",
    "        # 1 0 0\n",
    "        # 1 1 0\n",
    "        # 1 1 1\n",
    "\n",
    "        for weights in self.decoders:\n",
    "\n",
    "            Q, K, V = compute_qkv(x, weights['W_q'], weights['W_k'], weights['W_v'])\n",
    "            att = multi_head_attention(Q, K, V, weights['W_o'], self.n_heads, mask=mask, dropout=self.dropout)\n",
    "            if self.dropout: att = self.dropout.execute(att)\n",
    "            x = layer_normalization(x + att, weights['gamma'], weights['beta'])\n",
    "\n",
    "            Q = x @ weights['W_q_cross']\n",
    "            K = enc_out @ weights['W_k_cross']\n",
    "            V = enc_out @ weights['W_v_cross']\n",
    "            cross_att = multi_head_attention(Q, K, V, weights['W_o_cross'], self.n_heads, mask=None, dropout=self.dropout)\n",
    "            if self.dropout: cross_att = self.dropout.execute(cross_att)\n",
    "            x = layer_normalization(x + cross_att, weights['gamma'], weights['beta'])\n",
    "            \n",
    "            ff = relu(x @ weights['W_ff1'] + weights['bias_ff1']) @ weights['W_ff2'] + weights['bias_ff2']\n",
    "            if self.dropout: ff = self.dropout.execute(ff)\n",
    "            x = layer_normalization(x + ff, weights['gamma'], weights['beta'])\n",
    "\n",
    "        return x @ self.embedding.T # seq_len x vocab_size\n",
    "    \n",
    "    def generate(self, src, start_token, max_len):\n",
    "        enc_out = self.run_encoders(src)\n",
    "\n",
    "        output = [start_token]\n",
    "        for _ in range(max_len):\n",
    "            logits = self.run_decoders(enc_out, output)\n",
    "            probs = softmax(logits[-1])\n",
    "            token = np.argmax(probs)\n",
    "            output.append(token.item())\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def eval(self):\n",
    "        self.dropout = None\n",
    "    def train(self):\n",
    "        self.dropout = Dropout(p=self.dropout_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98d863ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 13 12  9 17 17  8 10  4  6] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[ 9 12 18 17  1  6 12 11  7  4] [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "[16  6 13  6 12 12  3 14  5 14] [0, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "[19 13  7  3  6 18  3  1 15  8] [0, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "[ 6 10  1 10  2  2 18 10 14  1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[16 19  6  9 13 16  8 16  5 19] [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "[ 4 10  6 15  5 16  8 12  6 19] [0, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "[11  9 19 12  2  2  1 12 18  3] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[ 4  7 12 11 19  2 10  2  9  9] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[ 8  3  2 17 14 15 15  4  5 10] [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "model = Transformer(vocab_size=21, d_model=128, n_heads=4, d_ff=256, n_blocks=2)\n",
    "model.eval()\n",
    "\n",
    "for _ in range(10):\n",
    "    src = np.random.randint(1, 20, (10))\n",
    "    print(src, model.generate(src, 0, len(src)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
